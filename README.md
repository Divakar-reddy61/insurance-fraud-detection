# Insurance Fraud Detection - Vehicle Damage Analysis

A deep learning-based system for detecting fraudulent vehicle damage claims by analyzing images using CNN models. This project uses image classification to distinguish between real damage (authentic claims) and fake/manipulated images (potential fraud).

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [Data Processing Pipeline](#data-processing-pipeline)
  - [Model Training](#model-training)
  - [Running the Flask Web App](#running-the-flask-web-app)
- [Dataset](#dataset)
- [Technologies](#technologies)
- [Model Architecture](#model-architecture)
- [API Endpoints](#api-endpoints)
- [Database Schema](#database-schema)
- [Results & Evaluation](#results--evaluation)
- [Contributing](#contributing)

---

## ğŸ¯ Project Overview

Insurance fraud detection through vehicle damage image analysis involves:

1. **Data Cleaning & Preprocessing**: Remove corrupted images, duplicates, and blurry images
2. **Exploratory Data Analysis (EDA)**: Analyze image characteristics and distributions
3. **Feature Extraction**: Extract features using CNN layers
4. **Model Training**: Train a custom CNN to classify real vs. fake damage
5. **Web Interface**: Interactive dashboard for predictions and visualization
6. **Database Integration**: Store predictions and analysis results

The system helps insurance companies reduce fraud by automating the verification of vehicle damage claims.

---

## âœ¨ Features

- **Automated Image Cleaning**: Removes duplicates using perceptual hashing, detects blurry images, validates image integrity
- **Exploratory Data Analysis**: Class distribution, image dimensions, brightness analysis, corruption detection
- **Custom CNN Model**: Built from scratch for vehicle damage classification
- **Feature Visualization**: Displays convolutional layer outputs to explain model decisions
- **Web Dashboard**: User-friendly Flask interface for uploading and analyzing images
- **Database Storage**: MySQL integration for storing predictions and claim data
- **Explainability**: Automatic explanations based on prediction confidence

---

## ğŸ“ Project Structure

```
INSURENCEFRAUDDETECTION/
â”œâ”€â”€ app.py                          # Flask web application
â”œâ”€â”€ main.py                         # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ db_config.py                    # Database configuration
â”œâ”€â”€ db_operations.py                # Database CRUD operations
â”œâ”€â”€ test_mysql.py                   # Database connection test
â”‚
â”œâ”€â”€ src/                            # Source code directory
â”‚   â”œâ”€â”€ dataload.py                # Load raw dataset
â”‚   â”œâ”€â”€ dataclean.py               # Data cleaning operations
â”‚   â”œâ”€â”€ EDA.py                     # Exploratory Data Analysis
â”‚   â”œâ”€â”€ train.py                   # Model training pipeline
â”‚   â”œâ”€â”€ evaluate.py                # Model evaluation metrics
â”‚   â”œâ”€â”€ predict.py                 # Prediction utilities (empty)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ notebook/                       # Jupyter notebooks
â”‚   â”œâ”€â”€ dataclean.ipynb            # Data cleaning notebook
â”‚   â”œâ”€â”€ dataload.ipynb             # Data loading notebook
â”‚   â”œâ”€â”€ datapreprocessing.ipynb    # Preprocessing notebook
â”‚   â””â”€â”€ train_test.ipynb           # Training & testing notebook
â”‚
â”œâ”€â”€ dataset/                        # Dataset directory
â”‚   â”œâ”€â”€ raw/                       # Raw dataset
â”‚   â”‚   â””â”€â”€ vehicle_damage_dataset/
â”‚   â”‚       â”œâ”€â”€ real/              # Real damage images
â”‚   â”‚       â”œâ”€â”€ fake/              # Fake/manipulated images
â”‚   â”‚       â”œâ”€â”€ labels.csv         # Original labels
â”‚   â”‚       â”œâ”€â”€ clean_labels_final.csv
â”‚   â”‚       â”œâ”€â”€ labels_no_blur.csv
â”‚   â”‚       â””â”€â”€ ... (other processed labels)
â”‚   â”‚
â”‚   â””â”€â”€ processed/                 # Processed/cleaned datasets
â”‚       â”œâ”€â”€ labels_no_duplicates.csv
â”‚       â”œâ”€â”€ labels_no_blur.csv
â”‚       â”œâ”€â”€ labels_no_corrupted.csv
â”‚       â””â”€â”€ labels_clean.csv
â”‚
â”œâ”€â”€ saved_models/                   # Trained models
â”‚   â”œâ”€â”€ best_cnn_model.h5          # Best model during training
â”‚   â””â”€â”€ final_cnn_model.h5         # Final trained model
â”‚
â”œâ”€â”€ static/                         # Flask static files
â”‚   â”œâ”€â”€ uploads/                   # User-uploaded images
â”‚   â”œâ”€â”€ image/                     # Sample images
â”‚   â””â”€â”€ feature_maps/              # Convolutional layer visualizations
â”‚       â”œâ”€â”€ conv_layer_1/
â”‚       â”œâ”€â”€ conv_layer_2/
â”‚       â””â”€â”€ conv_layer_3/
â”‚
â””â”€â”€ templates/                      # Flask HTML templates
    â”œâ”€â”€ index.html                 # Home page
    â”œâ”€â”€ dashboard.html             # Prediction dashboard
    â””â”€â”€ database.html              # Database records view
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.8+
- MySQL Server (for database functionality)
- Git

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd INSURENCEFRAUDDETECTION
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Configure Database

Edit `db_config.py` with your MySQL credentials:

```python
def get_db_connection():
    return mysql.connector.connect(
        host="localhost",
        user="your_username",
        password="your_password",
        database="insurance_db"
    )
```

### Step 5: Test Database Connection

```bash
python test_mysql.py
```

---

## ğŸš€ Usage

### Data Processing Pipeline

#### Full Pipeline Execution

Run the complete pipeline from data loading to model evaluation:

```bash
python main.py
```

This will:
1. Load the raw dataset
2. Clean and preprocess images
3. Run exploratory data analysis
4. Train the CNN model
5. Evaluate model performance

#### Individual Steps

**Data Loading:**
```python
from src.dataload import load_dataset
df, csv_path = load_dataset()
```

**Data Cleaning:**
```python
from src.dataclean import clean_dataset
clean_csv_path = clean_dataset()
```

**Run EDA:**
```python
from src.EDA import run_eda
run_eda()
```

### Model Training

```bash
python -c "from src.train import train_cnn_pipeline; train_cnn_pipeline()"
```

Or in Python:
```python
from src.train import train_cnn_pipeline
train_cnn_pipeline()
```

### Running the Flask Web App

```bash
python app.py
```

The web application will start at `http://localhost:5000`

**Features:**
- Upload vehicle damage images
- Get real-time predictions (Real vs. Fake)
- Visualize convolutional layer feature maps
- View prediction explanations
- Browse stored predictions in database

---

## ğŸ“Š Dataset

### Source Structure

```
dataset/raw/vehicle_damage_dataset/
â”œâ”€â”€ real/vehicle_damage/REAL/         # Authentic damage images
â””â”€â”€ fake/vehicle_damage/FAKE/         # Manipulated/fake images
```

### Dataset Statistics

| Metric | Value |
|--------|-------|
| Total Images | ~10,000+ |
| Real Images | ~50% |
| Fake Images | ~50% |
| Image Size | 224Ã—224 (processed) |
| Format | PNG/JPG |

### Data Cleaning Steps

1. **Remove Duplicates**: Uses perceptual hashing (imagehash.phash)
2. **Remove Corrupted Images**: Validates image integrity with PIL
3. **Remove Blurry Images**: Laplacian variance threshold (< 100)
4. **Verify Paths**: Ensures all image files exist

---

## ğŸ“¥ Downloads

### Dataset

- **Vehicle Damage Dataset (Google Drive)**: [Download dataset](https://drive.google.com/drive/folders/1sCAj8d_CnVjKnXkuurI1e9nef3nJT-Z7?usp=drive_link)
  - Contains real and fake vehicle damage images organized under `real/` and `fake/` folders
  - Size: (please verify in Drive)
  - Usage: place the unzipped dataset under `dataset/raw/vehicle_damage_dataset/` to match project paths

### Pre-trained Models (optional)

- **Best CNN Model**: stored locally in `saved_models/best_cnn_model.h5` (not tracked in repo)
- **Final CNN Model**: stored locally in `saved_models/final_cnn_model.h5` (not tracked in repo)

## ğŸ›  Technologies

| Technology | Purpose |
|-----------|---------|
| **TensorFlow/Keras** | Deep learning framework |
| **OpenCV** | Image processing |
| **Pandas** | Data manipulation |
| **NumPy** | Numerical computations |
| **Scikit-learn** | ML utilities & metrics |
| **Flask** | Web framework |
| **MySQL** | Database |
| **Matplotlib/Seaborn** | Visualization |
| **PIL/Pillow** | Image operations |
| **imagehash** | Duplicate detection |

---

## ğŸ§  Model Architecture

### CNN Model

```
Sequential Model:
â”œâ”€â”€ Conv2D(32, 3Ã—3, ReLU)
â”œâ”€â”€ MaxPooling2D(2Ã—2)
â”œâ”€â”€ Conv2D(64, 3Ã—3, ReLU)
â”œâ”€â”€ MaxPooling2D(2Ã—2)
â”œâ”€â”€ Conv2D(128, 3Ã—3, ReLU)
â”œâ”€â”€ MaxPooling2D(2Ã—2)
â”œâ”€â”€ Flatten()
â”œâ”€â”€ Dense(256, ReLU) + Dropout(0.5)
â”œâ”€â”€ Dense(128, ReLU) + Dropout(0.5)
â””â”€â”€ Dense(1, Sigmoid)  # Binary classification
```

### Training Configuration

- **Optimizer**: Adam (learning rate: 0.001)
- **Loss Function**: Binary Crossentropy
- **Metrics**: Accuracy, Precision, Recall
- **Batch Size**: 32
- **Epochs**: 10 (with early stopping)
- **Data Split**: 70% train, 15% validation, 15% test

---

## ğŸŒ API Endpoints

### Flask Routes

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Home page |
| `/dashboard` | GET | Prediction dashboard |
| `/predict` | POST | Upload image and get prediction |
| `/database` | GET | View all predictions |
| `/get_feature_maps/<image_id>` | GET | Get feature visualizations |

### Prediction Response

```json
{
  "prediction": 0.85,
  "class": "REAL",
  "confidence": "85%",
  "explanation": [
    "Input image resized to 224Ã—224 and normalized.",
    "Strong structural consistency detected.",
    "Damage patterns match real accident images.",
    "High confidence in image authenticity.",
    "CNN combined multi-level features to make final decision."
  ],
  "feature_maps": {
    "conv_layer_1": ["static/feature_maps/conv_layer_1/0.png", ...],
    "conv_layer_2": [...],
    "conv_layer_3": [...]
  }
}
```

---

## ğŸ—„ Database Schema

### Table: `predictions`

```sql
CREATE TABLE predictions (
  id INT AUTO_INCREMENT PRIMARY KEY,
  image_name VARCHAR(255) NOT NULL,
  prediction_value FLOAT NOT NULL,
  predicted_class VARCHAR(50) NOT NULL,
  confidence FLOAT NOT NULL,
  upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  explanation TEXT
);
```

### Database Operations

- **Save Prediction**: `save_prediction(image_name, pred, class_label, confidence)`
- **Fetch Predictions**: `fetch_predictions(limit=100)`
- **Database Connection**: `get_db_connection()`

---

## ğŸ“ˆ Results & Evaluation

### Model Performance Metrics

The model is evaluated using:

- **Accuracy**: Overall correctness
- **Precision**: True positive rate among positives
- **Recall**: Detection rate of actual frauds
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under the receiver operating characteristic curve
- **Confusion Matrix**: TP, TN, FP, FN distribution

### Evaluation Report

Run evaluation:

```bash
python -c "from src.evaluate import run_evaluation; run_evaluation()"
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ Project Workflow

### Typical User Journey

1. **Start Application**
   ```bash
   python app.py
   ```

2. **Upload Vehicle Damage Image**
   - Navigate to `/dashboard`
   - Upload image (JPG/PNG)

3. **Get Prediction**
   - Model processes image
   - Returns classification: REAL or FAKE
   - Shows confidence level (0-100%)

4. **View Feature Maps**
   - Displays layer activations
   - Explains what model detected

5. **Review in Database**
   - View historical predictions
   - Analyze fraud patterns
   - Export reports

---

## âš ï¸ Important Notes

- **Database Password**: Change default credentials in `db_config.py`
- **Model Path**: Update paths if using different directory structures
- **Image Size**: Model expects 224Ã—224 images; preprocessing handles resizing
- **GPU Support**: Install `tensorflow-gpu` for faster training if GPU available

---

## ğŸ“ Support & Contact

For issues or questions:
- Review the Jupyter notebooks for detailed examples
- Check the `src/` modules for function documentation
- Ensure MySQL server is running before using database features

---

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ” Key Features Summary

| Feature | Status |
|---------|--------|
| Data cleaning pipeline | âœ… Complete |
| EDA & visualization | âœ… Complete |
| CNN model training | âœ… Complete |
| Model evaluation | âœ… Complete |
| Flask web interface | âœ… Complete |
| Database integration | âœ… Complete |
| Feature visualization | âœ… Complete |
| Prediction explanations | âœ… Complete |
| API endpoints | âœ… Complete |

---

**Last Updated**: January 2026

**Project Status**: Active Development
